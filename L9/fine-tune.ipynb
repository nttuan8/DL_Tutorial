{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Thêm thư viện\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.applications import VGG16\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Flatten\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy các đường dẫn đến ảnh.\n",
    "image_path = list(paths.list_images('dataset/'))\n",
    "\n",
    "# Đổi vị trí ngẫu nhiên các đường dẫn ảnh\n",
    "random.shuffle(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đường dẫn ảnh sẽ là dataset/tên_loài_hoa/tên_ảnh ví dụ dataset/Bluebell/image_0241.jpg nên p.split(os.path.sep)[-2] sẽ lấy ra được tên loài hoa\n",
    "labels = [p.split(os.path.sep)[-2] for p in image_path]\n",
    "\n",
    "# Chuyển tên các loài hoa thành số\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "\n",
    "# One-hot encoding\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ảnh và resize về đúng kích thước mà VGG 16 cần là (224,224)\n",
    "list_image = []\n",
    "for (j, imagePath) in enumerate(image_path):\n",
    "    image = load_img(imagePath, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    \n",
    "    image = np.expand_dims(image, 0)\n",
    "    image = imagenet_utils.preprocess_input(image)\n",
    "    \n",
    "    list_image.append(image)\n",
    "    \n",
    "list_image = np.vstack(list_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model VGG 16 của ImageNet dataset, include_top=False để bỏ phần Fully connected layer ở cuối.\n",
    "baseModel = VGG16(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# Xây thêm các layer\n",
    "# Lấy output của ConvNet trong VGG16\n",
    "fcHead = baseModel.output\n",
    "\n",
    "# Flatten trước khi dùng FCs\n",
    "fcHead = Flatten(name='flatten')(fcHead)\n",
    "\n",
    "# Thêm FC\n",
    "fcHead = Dense(256, activation='relu')(fcHead)\n",
    "fcHead = Dropout(0.5)(fcHead)\n",
    "\n",
    "# Output layer với softmax activation\n",
    "fcHead = Dense(17, activation='softmax')(fcHead)\n",
    "\n",
    "# Xây dựng model bằng việc nối ConvNet của VGG16 và fcHead\n",
    "model = model = Model(inputs=baseModel.input, outputs=fcHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia traing set, test set tỉ lệ 80-20\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_image, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation cho training data\n",
    "aug_train = ImageDataGenerator(rescale=1./255, rotation_range=30, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, \n",
    "                         zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "# augementation cho test\n",
    "aug_test= ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "34/34 [==============================] - 36s 1s/step - loss: 5.8708 - acc: 0.1443 - val_loss: 1.8312 - val_acc: 0.4805\n",
      "Epoch 2/25\n",
      "34/34 [==============================] - 25s 727ms/step - loss: 2.2167 - acc: 0.3097 - val_loss: 1.4920 - val_acc: 0.4833\n",
      "Epoch 3/25\n",
      "34/34 [==============================] - 21s 631ms/step - loss: 1.8023 - acc: 0.4292 - val_loss: 1.2073 - val_acc: 0.6583\n",
      "Epoch 4/25\n",
      "34/34 [==============================] - 21s 632ms/step - loss: 1.5419 - acc: 0.5064 - val_loss: 0.9879 - val_acc: 0.7208\n",
      "Epoch 5/25\n",
      "34/34 [==============================] - 22s 634ms/step - loss: 1.4261 - acc: 0.5506 - val_loss: 0.6725 - val_acc: 0.8083\n",
      "Epoch 6/25\n",
      "34/34 [==============================] - 22s 633ms/step - loss: 1.3022 - acc: 0.5873 - val_loss: 0.7243 - val_acc: 0.7667\n",
      "Epoch 7/25\n",
      "34/34 [==============================] - 22s 636ms/step - loss: 1.1401 - acc: 0.6241 - val_loss: 0.6351 - val_acc: 0.8708\n",
      "Epoch 8/25\n",
      "34/34 [==============================] - 22s 636ms/step - loss: 1.1156 - acc: 0.6480 - val_loss: 0.5055 - val_acc: 0.8500\n",
      "Epoch 9/25\n",
      "34/34 [==============================] - 22s 636ms/step - loss: 1.0335 - acc: 0.6498 - val_loss: 0.4785 - val_acc: 0.8417\n",
      "Epoch 10/25\n",
      "34/34 [==============================] - 22s 638ms/step - loss: 0.9727 - acc: 0.6949 - val_loss: 0.4631 - val_acc: 0.8711\n",
      "Epoch 11/25\n",
      "34/34 [==============================] - 22s 638ms/step - loss: 0.9333 - acc: 0.7050 - val_loss: 0.5691 - val_acc: 0.8417\n",
      "Epoch 12/25\n",
      "34/34 [==============================] - 22s 638ms/step - loss: 0.8679 - acc: 0.7243 - val_loss: 0.5470 - val_acc: 0.8125\n",
      "Epoch 13/25\n",
      "34/34 [==============================] - 22s 639ms/step - loss: 0.8397 - acc: 0.7335 - val_loss: 0.5751 - val_acc: 0.8000\n",
      "Epoch 14/25\n",
      "34/34 [==============================] - 22s 639ms/step - loss: 0.8081 - acc: 0.7491 - val_loss: 0.5523 - val_acc: 0.8333\n",
      "Epoch 15/25\n",
      "34/34 [==============================] - 22s 643ms/step - loss: 0.7724 - acc: 0.7629 - val_loss: 0.4636 - val_acc: 0.8750\n",
      "Epoch 16/25\n",
      "34/34 [==============================] - 22s 640ms/step - loss: 0.7494 - acc: 0.7509 - val_loss: 0.3266 - val_acc: 0.9083\n",
      "Epoch 17/25\n",
      "34/34 [==============================] - 22s 641ms/step - loss: 0.7434 - acc: 0.7445 - val_loss: 0.4980 - val_acc: 0.8625\n",
      "Epoch 18/25\n",
      "34/34 [==============================] - 22s 638ms/step - loss: 0.6952 - acc: 0.7693 - val_loss: 0.3791 - val_acc: 0.8917\n",
      "Epoch 19/25\n",
      "34/34 [==============================] - 22s 637ms/step - loss: 0.6805 - acc: 0.7739 - val_loss: 0.4193 - val_acc: 0.8633\n",
      "Epoch 20/25\n",
      "34/34 [==============================] - 22s 640ms/step - loss: 0.6195 - acc: 0.7858 - val_loss: 0.4143 - val_acc: 0.8750\n",
      "Epoch 21/25\n",
      "34/34 [==============================] - 22s 639ms/step - loss: 0.6625 - acc: 0.7849 - val_loss: 0.3923 - val_acc: 0.8958\n",
      "Epoch 22/25\n",
      "34/34 [==============================] - 22s 636ms/step - loss: 0.5939 - acc: 0.8079 - val_loss: 0.4184 - val_acc: 0.8833\n",
      "Epoch 23/25\n",
      "34/34 [==============================] - 22s 643ms/step - loss: 0.5655 - acc: 0.8171 - val_loss: 0.3251 - val_acc: 0.9042\n",
      "Epoch 24/25\n",
      "34/34 [==============================] - 22s 637ms/step - loss: 0.6053 - acc: 0.8079 - val_loss: 0.5748 - val_acc: 0.8542\n",
      "Epoch 25/25\n",
      "34/34 [==============================] - 22s 636ms/step - loss: 0.5360 - acc: 0.8208 - val_loss: 0.3918 - val_acc: 0.9000\n"
     ]
    }
   ],
   "source": [
    "# freeze VGG model\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "opt = RMSprop(0.001)\n",
    "model.compile(opt, 'categorical_crossentropy', ['accuracy'])\n",
    "numOfEpoch = 25\n",
    "H = model.fit_generator(aug_train.flow(X_train, y_train, batch_size=32), \n",
    "                        steps_per_epoch=len(X_train)//32,\n",
    "                        validation_data=(aug_test.flow(X_test, y_test, batch_size=32)),\n",
    "                        validation_steps=len(X_test)//32,\n",
    "                        epochs=numOfEpoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "34/34 [==============================] - 26s 771ms/step - loss: 0.4242 - acc: 0.8603 - val_loss: 0.3123 - val_acc: 0.9258\n",
      "Epoch 2/35\n",
      "34/34 [==============================] - 24s 712ms/step - loss: 0.3769 - acc: 0.8814 - val_loss: 0.2623 - val_acc: 0.9250\n",
      "Epoch 3/35\n",
      "34/34 [==============================] - 24s 709ms/step - loss: 0.3101 - acc: 0.9062 - val_loss: 0.2925 - val_acc: 0.9375\n",
      "Epoch 4/35\n",
      "34/34 [==============================] - 24s 704ms/step - loss: 0.2902 - acc: 0.9035 - val_loss: 0.4253 - val_acc: 0.9292\n",
      "Epoch 5/35\n",
      "34/34 [==============================] - 24s 705ms/step - loss: 0.2829 - acc: 0.8961 - val_loss: 0.3271 - val_acc: 0.9417\n",
      "Epoch 6/35\n",
      "34/34 [==============================] - 24s 707ms/step - loss: 0.2556 - acc: 0.9191 - val_loss: 0.1768 - val_acc: 0.9250\n",
      "Epoch 7/35\n",
      "34/34 [==============================] - 24s 706ms/step - loss: 0.2351 - acc: 0.9228 - val_loss: 0.3223 - val_acc: 0.9375\n",
      "Epoch 8/35\n",
      "34/34 [==============================] - 24s 707ms/step - loss: 0.2641 - acc: 0.9118 - val_loss: 0.3800 - val_acc: 0.9292\n",
      "Epoch 9/35\n",
      "34/34 [==============================] - 24s 708ms/step - loss: 0.2760 - acc: 0.9173 - val_loss: 0.2979 - val_acc: 0.9458\n",
      "Epoch 10/35\n",
      "34/34 [==============================] - 24s 710ms/step - loss: 0.2466 - acc: 0.9090 - val_loss: 0.3352 - val_acc: 0.9453\n",
      "Epoch 11/35\n",
      "34/34 [==============================] - 24s 709ms/step - loss: 0.2247 - acc: 0.9210 - val_loss: 0.2563 - val_acc: 0.9500\n",
      "Epoch 12/35\n",
      "34/34 [==============================] - 24s 709ms/step - loss: 0.2423 - acc: 0.9274 - val_loss: 0.3930 - val_acc: 0.9417\n",
      "Epoch 13/35\n",
      "34/34 [==============================] - 24s 709ms/step - loss: 0.2042 - acc: 0.9311 - val_loss: 0.2740 - val_acc: 0.9542\n",
      "Epoch 14/35\n",
      "34/34 [==============================] - 24s 709ms/step - loss: 0.2141 - acc: 0.9256 - val_loss: 0.3703 - val_acc: 0.9375\n",
      "Epoch 15/35\n",
      "34/34 [==============================] - 24s 705ms/step - loss: 0.2355 - acc: 0.9191 - val_loss: 0.2008 - val_acc: 0.9625\n",
      "Epoch 16/35\n",
      "34/34 [==============================] - 24s 707ms/step - loss: 0.2310 - acc: 0.9182 - val_loss: 0.2346 - val_acc: 0.9500\n",
      "Epoch 17/35\n",
      "34/34 [==============================] - 24s 708ms/step - loss: 0.1893 - acc: 0.9329 - val_loss: 0.3341 - val_acc: 0.9542\n",
      "Epoch 18/35\n",
      "34/34 [==============================] - 24s 709ms/step - loss: 0.2123 - acc: 0.9256 - val_loss: 0.3322 - val_acc: 0.9458\n",
      "Epoch 19/35\n",
      "34/34 [==============================] - 24s 710ms/step - loss: 0.1802 - acc: 0.9375 - val_loss: 0.2476 - val_acc: 0.9570\n",
      "Epoch 20/35\n",
      "34/34 [==============================] - 24s 705ms/step - loss: 0.1902 - acc: 0.9366 - val_loss: 0.3922 - val_acc: 0.9375\n",
      "Epoch 21/35\n",
      "34/34 [==============================] - 24s 708ms/step - loss: 0.1852 - acc: 0.9384 - val_loss: 0.2375 - val_acc: 0.9583\n",
      "Epoch 22/35\n",
      "34/34 [==============================] - 24s 709ms/step - loss: 0.1782 - acc: 0.9458 - val_loss: 0.2803 - val_acc: 0.9500\n",
      "Epoch 23/35\n",
      "34/34 [==============================] - 24s 709ms/step - loss: 0.2021 - acc: 0.9347 - val_loss: 0.3591 - val_acc: 0.9208\n",
      "Epoch 24/35\n",
      "34/34 [==============================] - 24s 705ms/step - loss: 0.2031 - acc: 0.9265 - val_loss: 0.4288 - val_acc: 0.9292\n",
      "Epoch 25/35\n",
      "34/34 [==============================] - 24s 707ms/step - loss: 0.1587 - acc: 0.9449 - val_loss: 0.2866 - val_acc: 0.9333\n",
      "Epoch 26/35\n",
      "34/34 [==============================] - 24s 708ms/step - loss: 0.1837 - acc: 0.9375 - val_loss: 0.3268 - val_acc: 0.9417\n",
      "Epoch 27/35\n",
      "34/34 [==============================] - 24s 708ms/step - loss: 0.1921 - acc: 0.9384 - val_loss: 0.2889 - val_acc: 0.9458\n",
      "Epoch 28/35\n",
      "34/34 [==============================] - 24s 708ms/step - loss: 0.1747 - acc: 0.9375 - val_loss: 0.3303 - val_acc: 0.9492\n",
      "Epoch 29/35\n",
      "34/34 [==============================] - 24s 705ms/step - loss: 0.1627 - acc: 0.9485 - val_loss: 0.3628 - val_acc: 0.9500\n",
      "Epoch 30/35\n",
      "34/34 [==============================] - 24s 706ms/step - loss: 0.1721 - acc: 0.9393 - val_loss: 0.2802 - val_acc: 0.9333\n",
      "Epoch 31/35\n",
      "34/34 [==============================] - 24s 707ms/step - loss: 0.1788 - acc: 0.9421 - val_loss: 0.2832 - val_acc: 0.9375\n",
      "Epoch 32/35\n",
      "34/34 [==============================] - 24s 707ms/step - loss: 0.1880 - acc: 0.9375 - val_loss: 0.3612 - val_acc: 0.9417\n",
      "Epoch 33/35\n",
      "34/34 [==============================] - 24s 708ms/step - loss: 0.1932 - acc: 0.9320 - val_loss: 0.4107 - val_acc: 0.9417\n",
      "Epoch 34/35\n",
      "34/34 [==============================] - 24s 708ms/step - loss: 0.1603 - acc: 0.9485 - val_loss: 0.2739 - val_acc: 0.9500\n",
      "Epoch 35/35\n",
      "34/34 [==============================] - 24s 707ms/step - loss: 0.1168 - acc: 0.9632 - val_loss: 0.1849 - val_acc: 0.9667\n"
     ]
    }
   ],
   "source": [
    "# unfreeze some last CNN layer:\n",
    "for layer in baseModel.layers[15:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "numOfEpoch = 35\n",
    "opt = SGD(0.001)\n",
    "model.compile(opt, 'categorical_crossentropy', ['accuracy'])\n",
    "H = model.fit_generator(aug_train.flow(X_train, y_train, batch_size=32), \n",
    "                        steps_per_epoch=len(X_train)//32,\n",
    "                        validation_data=(aug_test.flow(X_test, y_test, batch_size=32)),\n",
    "                        validation_steps=len(X_test)//32,\n",
    "                        epochs=numOfEpoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
